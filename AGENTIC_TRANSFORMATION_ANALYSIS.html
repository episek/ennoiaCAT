<h1>EnnoiaCAT Agentic Framework Transformation Analysis</h1>
<p><strong>Document Version:</strong> 1.0
<strong>Date:</strong> 2025-12-22
<strong>Project:</strong> EnnoiaCAT Multi-Instrument Test Platform
<strong>Prepared By:</strong> Claude Code Analysis</p>
<hr />
<h2>Executive Summary</h2>
<p>This document provides a comprehensive analysis of transforming the EnnoiaCAT multi-instrument test platform into a full-fledged <strong>agentic framework</strong> with Model Context Protocol (MCP) server integration. The EnnoiaCAT platform already demonstrates significant agentic capabilities through its <code>ennoia_agentic_app.py</code> implementation. This analysis outlines how to systematically expand these capabilities into a comprehensive, production-grade agentic framework.</p>
<p><strong>Key Findings:</strong>
- <strong>Current State:</strong> Partial agentic implementation with 3 specialized agents (Configuration, Location, Analysis)
- <strong>Transformation Scope:</strong> Medium-High complexity (6-8 weeks for full implementation)
- <strong>ROI:</strong> High - significant improvements in automation, scalability, and user experience
- <strong>Risk Level:</strong> Medium - requires careful refactoring while maintaining existing functionality</p>
<hr />
<h2>Table of Contents</h2>
<ol>
<li><a href="#1-current-architecture-analysis">Current Architecture Analysis</a></li>
<li><a href="#2-agentic-framework-vision">Agentic Framework Vision</a></li>
<li><a href="#3-benefits-assessment">Benefits Assessment</a></li>
<li><a href="#4-complexity--risk-analysis">Complexity &amp; Risk Analysis</a></li>
<li><a href="#5-mcp-server-integration-strategy">MCP Server Integration Strategy</a></li>
<li><a href="#6-step-by-step-execution-plan">Step-by-Step Execution Plan</a></li>
<li><a href="#7-implementation-milestones">Implementation Milestones</a></li>
<li><a href="#8-success-metrics">Success Metrics</a></li>
<li><a href="#9-appendices">Appendices</a></li>
</ol>
<hr />
<h2>1. Current Architecture Analysis</h2>
<h3>1.1 Existing Agentic Components</h3>
<p>The platform already contains foundational agentic elements in <code>ennoia_agentic_app.py</code>:</p>
<h4><strong>Agent 1: ConfigurationAgent</strong></h4>
<pre><code class="language-python">class ConfigurationAgent:
    - Purpose: Device selection, AI mode configuration (SLM/LLM)
    - Responsibilities: Model loading, checkbox selection, port detection
    - Current State: Basic autonomous decision-making
</code></pre>
<h4><strong>Agent 2: LocationAgent</strong></h4>
<pre><code class="language-python">class LocationAgent:
    - Purpose: Geolocation detection, operator frequency table building
    - Responsibilities: SSH/SFTP to EC2, remote script execution
    - Current State: Advanced automation with remote execution
</code></pre>
<h4><strong>Agent 3: AnalysisAgent</strong></h4>
<pre><code class="language-python">class AnalysisAgent:
    - Purpose: Cellular + WiFi spectrum analysis
    - Responsibilities: Multi-step data processing, agentic reasoning
    - Current State: Complex multi-step workflows
</code></pre>
<h3>1.2 Current Limitations</h3>
<table>
<thead>
<tr>
<th>Limitation</th>
<th>Impact</th>
<th>Priority</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Agent Isolation</strong></td>
<td>Agents operate in silos with minimal inter-agent communication</td>
<td>High</td>
</tr>
<tr>
<td><strong>No Central Orchestration</strong></td>
<td>Workflows are hardcoded in sequence (Config → Location → Analysis)</td>
<td>High</td>
</tr>
<tr>
<td><strong>Limited Autonomy</strong></td>
<td>Agents require user input at each stage</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>No Agent Discovery</strong></td>
<td>Cannot dynamically add/remove agents</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>Single Execution Path</strong></td>
<td>Linear workflow, no branching or parallel execution</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>No Agent Memory</strong></td>
<td>Agents don't maintain state across sessions</td>
<td>Low</td>
</tr>
<tr>
<td><strong>Minimal Tool Sharing</strong></td>
<td>Each agent has isolated tool sets</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<h3>1.3 Strengths to Preserve</h3>
<ul>
<li><strong>Adapter Pattern</strong>: Excellent abstraction for 7+ instrument types</li>
<li><strong>Dual AI Stack</strong>: Both SLM (offline) and LLM (online) support</li>
<li><strong>Modular Config System</strong>: Instrument-specific helpers are well-isolated</li>
<li><strong>Remote Execution</strong>: SSH/SFTP capability for distributed computing</li>
<li><strong>Comprehensive Testing</strong>: pytest framework with unit/integration tests</li>
</ul>
<hr />
<h2>2. Agentic Framework Vision</h2>
<h3>2.1 Target Architecture</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                    STREAMLIT UI LAYER                            │
│            (Natural Language Interface + Dashboards)             │
└────────────────────────┬────────────────────────────────────────┘
                         │
┌────────────────────────┴────────────────────────────────────────┐
│                  AGENT ORCHESTRATION LAYER                       │
│                                                                  │
│  ┌────────────────────────────────────────────────────────┐    │
│  │         Master Orchestrator Agent                      │    │
│  │  - Task decomposition                                  │    │
│  │  - Agent routing &amp; coordination                        │    │
│  │  - Workflow planning &amp; execution                       │    │
│  │  - Error handling &amp; recovery                           │    │
│  └────────────────────────────────────────────────────────┘    │
│                                                                  │
│  ┌────────────────────────────────────────────────────────┐    │
│  │         Agent Registry &amp; Discovery Service             │    │
│  │  - Dynamic agent registration                          │    │
│  │  - Capability advertisement                            │    │
│  │  - Agent lifecycle management                          │    │
│  └────────────────────────────────────────────────────────┘    │
└────────────────────────┬────────────────────────────────────────┘
                         │
┌────────────────────────┴────────────────────────────────────────┐
│                    SPECIALIZED AGENT POOL                        │
│                                                                  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │ Instrument   │  │  Analysis    │  │  Planning    │          │
│  │ Control      │  │  Agent       │  │  Agent       │          │
│  │ Agents       │  │              │  │              │          │
│  │              │  │              │  │              │          │
│  │ • TinySA     │  │ • Spectrum   │  │ • Test Plan  │          │
│  │ • Viavi      │  │ • 5G NR      │  │ • Workflow   │          │
│  │ • Mavenir    │  │ • WiFi       │  │ • Resource   │          │
│  │ • Cisco      │  │ • Cellular   │  │              │          │
│  │ • Keysight   │  │ • Anomaly    │  │              │          │
│  │ • R&amp;S        │  │   Detection  │  │              │          │
│  │ • Aukua      │  │              │  │              │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
│                                                                  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │ Data         │  │ Location     │  │ Report       │          │
│  │ Processing   │  │ Agent        │  │ Generation   │          │
│  │ Agent        │  │              │  │ Agent        │          │
│  │              │  │ • Geolocation│  │              │          │
│  │ • Calibration│  │ • Operator   │  │ • PDF/HTML   │          │
│  │ • Filtering  │  │   DB         │  │ • Compliance │          │
│  │ • Transform  │  │ • Frequency  │  │ • Export     │          │
│  │              │  │   Mapping    │  │              │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
│                                                                  │
│  ┌──────────────────────────────────────────────────────┐      │
│  │         External MCP Agents (via MCP Client)          │      │
│  │  • Cloud Storage Agent (S3, Azure Blob)              │      │
│  │  • Database Agent (PostgreSQL, MongoDB)              │      │
│  │  • Notification Agent (Email, Slack, Teams)          │      │
│  │  • Third-party Analysis (external ML models)         │      │
│  │  • Compliance &amp; Audit Agents                         │      │
│  └──────────────────────────────────────────────────────┘      │
└────────────────────────┬────────────────────────────────────────┘
                         │
┌────────────────────────┴────────────────────────────────────────┐
│              AGENT COMMUNICATION LAYER                           │
│                                                                  │
│  ┌────────────────────────────────────────────────────────┐    │
│  │     Message Bus (Event-Driven Architecture)            │    │
│  │  - Agent-to-agent messaging                            │    │
│  │  - Event publishing/subscription                       │    │
│  │  - Task queuing                                        │    │
│  └────────────────────────────────────────────────────────┘    │
│                                                                  │
│  ┌────────────────────────────────────────────────────────┐    │
│  │     Shared Memory &amp; State Management                   │    │
│  │  - Session state (Streamlit st.session_state)          │    │
│  │  - Agent memory (short-term/long-term)                 │    │
│  │  - Context sharing                                     │    │
│  └────────────────────────────────────────────────────────┘    │
│                                                                  │
│  ┌────────────────────────────────────────────────────────┐    │
│  │     MCP Client Integration Layer                       │    │
│  │  - MCP protocol handler                                │    │
│  │  - External agent discovery                            │    │
│  │  - Capability negotiation                              │    │
│  │  - Tool/resource sharing                               │    │
│  └────────────────────────────────────────────────────────┘    │
└────────────────────────┬────────────────────────────────────────┘
                         │
┌────────────────────────┴────────────────────────────────────────┐
│                    SHARED TOOL LAYER                             │
│                                                                  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │ Instrument   │  │ AI/ML        │  │ Network      │          │
│  │ Adapters     │  │ Tools        │  │ Tools        │          │
│  │              │  │              │  │              │          │
│  │ • Factory    │  │ • Model Load │  │ • SSH/SFTP   │          │
│  │ • Detector   │  │ • Inference  │  │ • SCPI       │          │
│  │ • VISA       │  │ • MapAPI     │  │ • NETCONF    │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
└──────────────────────────────────────────────────────────────────┘
</code></pre>
<h3>2.2 Core Agentic Principles</h3>
<h4><strong>1. Autonomy</strong></h4>
<ul>
<li>Agents make independent decisions within their domain</li>
<li>Minimal human intervention required for routine tasks</li>
<li>Self-healing and error recovery capabilities</li>
</ul>
<h4><strong>2. Reactivity</strong></h4>
<ul>
<li>Agents respond to environment changes (instrument connections, data anomalies)</li>
<li>Event-driven architecture for real-time adaptation</li>
<li>Proactive monitoring and alerting</li>
</ul>
<h4><strong>3. Pro-activeness</strong></h4>
<ul>
<li>Agents anticipate user needs (e.g., auto-calibration before tests)</li>
<li>Goal-directed behavior (e.g., optimize test plans)</li>
<li>Continuous improvement through learning</li>
</ul>
<h4><strong>4. Social Ability</strong></h4>
<ul>
<li>Inter-agent communication for complex workflows</li>
<li>Negotiation and coordination protocols</li>
<li>Shared context and knowledge</li>
</ul>
<h3>2.3 Agent Taxonomy</h3>
<table>
<thead>
<tr>
<th>Agent Type</th>
<th>Responsibility</th>
<th>Example Agents</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Orchestrator</strong></td>
<td>Workflow management, task decomposition</td>
<td>MasterOrchestrator</td>
</tr>
<tr>
<td><strong>Instrument Controllers</strong></td>
<td>Device-specific operations</td>
<td>TinySAAgent, ViaviAgent, MavenirAgent</td>
</tr>
<tr>
<td><strong>Analysis Specialists</strong></td>
<td>Domain-specific analysis</td>
<td>SpectrumAnalyzer, 5GNRAnalyzer, WiFiAnalyzer</td>
</tr>
<tr>
<td><strong>Data Processors</strong></td>
<td>Data transformation &amp; validation</td>
<td>CalibrationAgent, FilterAgent</td>
</tr>
<tr>
<td><strong>Infrastructure</strong></td>
<td>System-level operations</td>
<td>LocationAgent, StorageAgent, ReportAgent</td>
</tr>
<tr>
<td><strong>External MCP Agents</strong></td>
<td>Third-party integrations</td>
<td>CloudStorageAgent, NotificationAgent</td>
</tr>
</tbody>
</table>
<hr />
<h2>3. Benefits Assessment</h2>
<h3>3.1 Quantitative Benefits</h3>
<table>
<thead>
<tr>
<th>Benefit Category</th>
<th>Current State</th>
<th>Target State</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Test Automation</strong></td>
<td>40% manual steps</td>
<td>85% automated</td>
<td>+113%</td>
</tr>
<tr>
<td><strong>Error Handling</strong></td>
<td>Manual recovery</td>
<td>Auto-retry + fallback</td>
<td>-70% downtime</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>1 test at a time</td>
<td>Parallel multi-instrument tests</td>
<td>5-10x throughput</td>
</tr>
<tr>
<td><strong>Setup Time</strong></td>
<td>15-20 min/test</td>
<td>2-3 min/test</td>
<td>-85% setup time</td>
</tr>
<tr>
<td><strong>User Expertise Required</strong></td>
<td>Expert (RF engineering)</td>
<td>Intermediate (guided workflows)</td>
<td>-60% training time</td>
</tr>
<tr>
<td><strong>Agent Reusability</strong></td>
<td>30% (3 agents)</td>
<td>90% (modular agents)</td>
<td>+200%</td>
</tr>
</tbody>
</table>
<h3>3.2 Qualitative Benefits</h3>
<h4><strong>For End Users</strong></h4>
<ol>
<li><strong>Natural Language Interaction</strong>: "Test 5G NR on band n77 in New York" → automatic test execution</li>
<li><strong>Reduced Cognitive Load</strong>: AI orchestrator handles complex multi-step workflows</li>
<li><strong>Faster Time-to-Results</strong>: Parallel test execution across multiple instruments</li>
<li><strong>Better Insights</strong>: Automatic anomaly detection and root cause analysis</li>
<li><strong>Workflow Persistence</strong>: Resume interrupted tests automatically</li>
</ol>
<h4><strong>For Developers</strong></h4>
<ol>
<li><strong>Faster Feature Development</strong>: Add new agent = add new capability (no core changes)</li>
<li><strong>Better Testing</strong>: Isolated agent testing vs. full integration testing</li>
<li><strong>Easier Debugging</strong>: Agent logs provide clear execution traces</li>
<li><strong>Code Reusability</strong>: Agents can be composed into new workflows</li>
<li><strong>Clear Separation of Concerns</strong>: Each agent has well-defined responsibilities</li>
</ol>
<h4><strong>For Enterprise</strong></h4>
<ol>
<li><strong>Horizontal Scaling</strong>: Add MCP server agents without code changes</li>
<li><strong>Multi-Tenancy Support</strong>: Different users → different agent configurations</li>
<li><strong>Compliance &amp; Auditability</strong>: Agent logs provide complete audit trails</li>
<li><strong>Vendor Integration</strong>: Easy integration with third-party test equipment via MCP</li>
<li><strong>Cloud-Native Architecture</strong>: Deploy agents as microservices</li>
</ol>
<h3>3.3 MCP-Specific Benefits</h3>
<h4><strong>Ecosystem Integration</strong></h4>
<ul>
<li><strong>Access to Pre-Built Agents</strong>: Leverage community-developed MCP agents</li>
<li>Example: Google Drive MCP server → automatic test report uploads</li>
<li>Example: Jira MCP server → automatic bug reporting from failed tests</li>
</ul>
<h4><strong>Flexibility</strong></h4>
<ul>
<li><strong>Mix Local + Remote Agents</strong>: Run heavy analysis on cloud, control instruments locally</li>
<li><strong>Language Agnostic</strong>: MCP agents can be written in any language</li>
<li><strong>Tool Sharing</strong>: External agents provide tools that your agents can use</li>
</ul>
<h4><strong>Future-Proofing</strong></h4>
<ul>
<li><strong>Standard Protocol</strong>: MCP is becoming industry standard (Anthropic, others)</li>
<li><strong>Vendor Independence</strong>: Switch between LLM providers without agent rewrites</li>
<li><strong>Open Ecosystem</strong>: Participate in MCP agent marketplace</li>
</ul>
<hr />
<h2>4. Complexity &amp; Risk Analysis</h2>
<h3>4.1 Complexity Breakdown</h3>
<h4><strong>Phase 1: Foundation (Low Complexity)</strong></h4>
<p><strong>Estimated Effort:</strong> 1-2 weeks
<strong>Risk Level:</strong> Low</p>
<ul>
<li>Extract existing agents into base classes</li>
<li>Create <code>BaseAgent</code> abstract class</li>
<li>Implement agent registry</li>
<li>Add basic message passing</li>
</ul>
<p><strong>Complexity Factors:</strong>
- Existing code is already modular ✓
- Python supports ABC (Abstract Base Classes) ✓
- No external dependencies required ✓</p>
<h4><strong>Phase 2: Orchestration (Medium Complexity)</strong></h4>
<p><strong>Estimated Effort:</strong> 2-3 weeks
<strong>Risk Level:</strong> Medium</p>
<ul>
<li>Implement <code>MasterOrchestrator</code> agent</li>
<li>Create workflow DSL (Domain-Specific Language)</li>
<li>Add task decomposition logic</li>
<li>Implement agent coordination protocols</li>
</ul>
<p><strong>Complexity Factors:</strong>
- Requires careful workflow design
- Need robust error handling
- May need async/await for parallel execution
- Testing orchestration logic is complex</p>
<h4><strong>Phase 3: MCP Integration (Medium-High Complexity)</strong></h4>
<p><strong>Estimated Effort:</strong> 2-3 weeks
<strong>Risk Level:</strong> Medium-High</p>
<ul>
<li>Implement MCP client library</li>
<li>Create MCP agent discovery service</li>
<li>Add MCP protocol handlers (JSON-RPC 2.0)</li>
<li>Implement tool/resource sharing with external agents</li>
</ul>
<p><strong>Complexity Factors:</strong>
- MCP protocol is well-documented ✓
- Need to handle network failures
- Security considerations (authentication, authorization)
- Multiple transport mechanisms (stdio, HTTP, WebSocket)</p>
<h4><strong>Phase 4: Advanced Features (High Complexity)</strong></h4>
<p><strong>Estimated Effort:</strong> 2-3 weeks
<strong>Risk Level:</strong> Medium</p>
<ul>
<li>Agent memory systems (RAG integration)</li>
<li>Learning from past test runs</li>
<li>Advanced error recovery</li>
<li>Performance optimization</li>
</ul>
<p><strong>Complexity Factors:</strong>
- Requires ML/AI expertise
- Database integration for memory
- Performance profiling needed</p>
<h3>4.2 Risk Matrix</h3>
<table>
<thead>
<tr>
<th>Risk</th>
<th>Likelihood</th>
<th>Impact</th>
<th>Mitigation Strategy</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Breaking existing workflows</strong></td>
<td>Medium</td>
<td>High</td>
<td>Incremental refactoring, maintain backward compatibility</td>
</tr>
<tr>
<td><strong>Performance degradation</strong></td>
<td>Low</td>
<td>Medium</td>
<td>Benchmark each phase, optimize hot paths</td>
</tr>
<tr>
<td><strong>MCP server unavailability</strong></td>
<td>Medium</td>
<td>Medium</td>
<td>Graceful fallback to local agents, caching</td>
</tr>
<tr>
<td><strong>Agent coordination deadlocks</strong></td>
<td>Low</td>
<td>High</td>
<td>Timeout mechanisms, circuit breakers</td>
</tr>
<tr>
<td><strong>Security vulnerabilities</strong></td>
<td>Medium</td>
<td>High</td>
<td>Agent sandboxing, input validation, MCP auth</td>
</tr>
<tr>
<td><strong>User adoption resistance</strong></td>
<td>Low</td>
<td>Medium</td>
<td>Maintain existing UI, gradual feature rollout</td>
</tr>
<tr>
<td><strong>Increased maintenance burden</strong></td>
<td>Medium</td>
<td>Medium</td>
<td>Comprehensive testing, clear documentation</td>
</tr>
</tbody>
</table>
<h3>4.3 Technical Challenges</h3>
<h4><strong>Challenge 1: Streamlit Event Loop</strong></h4>
<p><strong>Problem:</strong> Streamlit's execution model (top-to-bottom script rerun) conflicts with event-driven agents</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-python"># Use session state + asyncio for background agent execution
import asyncio
import streamlit as st

if &quot;event_loop&quot; not in st.session_state:
    st.session_state.event_loop = asyncio.new_event_loop()

# Agents run in background threads, update session state
</code></pre>
<h4><strong>Challenge 2: Agent State Persistence</strong></h4>
<p><strong>Problem:</strong> Streamlit reruns scripts on every interaction, losing agent state</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-python"># Singleton pattern + session state
class AgentRegistry:
    _instance = None

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
</code></pre>
<h4><strong>Challenge 3: Instrument Connection Management</strong></h4>
<p><strong>Problem:</strong> Network-based instruments (Viavi, Keysight) may have unstable connections</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-python"># Implement connection pooling + health checks
class ResilientInstrumentAdapter:
    def __init__(self):
        self.health_check_interval = 30  # seconds
        self.max_retries = 3

    async def execute_with_retry(self, command):
        for attempt in range(self.max_retries):
            try:
                return await self.execute(command)
            except ConnectionError:
                await self.reconnect()
</code></pre>
<hr />
<h2>5. MCP Server Integration Strategy</h2>
<h3>5.1 MCP Architecture Overview</h3>
<p><strong>Model Context Protocol (MCP)</strong> enables communication between AI agents and external services via a standardized protocol.</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                  EnnoiaCAT Application                       │
│                                                              │
│  ┌────────────────────────────────────────────────────┐    │
│  │           MCP Client Manager                       │    │
│  │  - Discovers available MCP servers                 │    │
│  │  - Manages connections to multiple servers         │    │
│  │  - Routes tool/resource requests                   │    │
│  └────────────────┬───────────────────────────────────┘    │
└───────────────────┼──────────────────────────────────────────┘
                    │ MCP Protocol
                    │ (JSON-RPC 2.0)
                    │
     ┌──────────────┼──────────────┬──────────────────┐
     │              │              │                  │
┌────▼────┐  ┌──────▼─────┐  ┌────▼────┐  ┌─────────▼────┐
│ Storage │  │ Database   │  │  Notify │  │   Custom     │
│ MCP     │  │ MCP        │  │  MCP    │  │   Analysis   │
│ Server  │  │ Server     │  │  Server │  │   MCP Server │
│         │  │            │  │         │  │              │
│ • S3    │  │ • Postgres │  │ • Email │  │ • Spectrum   │
│ • Azure │  │ • MongoDB  │  │ • Slack │  │   ML Model   │
│ • GCS   │  │ • Redis    │  │ • Teams │  │ • Anomaly    │
│         │  │            │  │         │  │   Detection  │
└─────────┘  └────────────┘  └─────────┘  └──────────────┘
</code></pre>
<h3>5.2 MCP Integration Patterns</h3>
<h4><strong>Pattern 1: Tool Augmentation</strong></h4>
<p>External MCP servers provide tools that local agents can call.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python"># Local SpectrumAnalysisAgent uses external ML model
class SpectrumAnalysisAgent(BaseAgent):
    async def analyze_spectrum(self, data):
        # Try MCP-based advanced ML model first
        mcp_tools = self.mcp_client.get_tools(&quot;spectrum-ml-server&quot;)

        if &quot;advanced_anomaly_detection&quot; in mcp_tools:
            result = await self.mcp_client.call_tool(
                server=&quot;spectrum-ml-server&quot;,
                tool=&quot;advanced_anomaly_detection&quot;,
                arguments={&quot;spectrum_data&quot;: data}
            )
            return result
        else:
            # Fallback to local analysis
            return self.local_analysis(data)
</code></pre>
<h4><strong>Pattern 2: Resource Sharing</strong></h4>
<p>MCP servers provide read-only resources (databases, file systems, APIs).</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python"># LocationAgent queries MCP database server for operator frequencies
class LocationAgent(BaseAgent):
    async def get_operator_frequencies(self, country, operator):
        # Query MCP database server
        result = await self.mcp_client.read_resource(
            server=&quot;spectrum-database-server&quot;,
            uri=f&quot;spectrum://operators/{country}/{operator}/frequencies&quot;
        )
        return result.contents
</code></pre>
<h4><strong>Pattern 3: Agent Delegation</strong></h4>
<p>Complex tasks delegated entirely to external MCP agents.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python"># Orchestrator delegates PDF report generation to MCP agent
class MasterOrchestrator(BaseAgent):
    async def generate_test_report(self, test_results):
        # Delegate to report generation MCP server
        prompt = f&quot;Generate compliance report for: {test_results}&quot;

        report = await self.mcp_client.create_message(
            server=&quot;report-generator-server&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            model=&quot;gpt-4&quot;  # MCP server chooses model
        )

        return report
</code></pre>
<h3>5.3 Recommended MCP Servers</h3>
<h4><strong>Tier 1: Essential (High Priority)</strong></h4>
<table>
<thead>
<tr>
<th>MCP Server</th>
<th>Purpose</th>
<th>Use Case in EnnoiaCAT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>@modelcontextprotocol/server-filesystem</strong></td>
<td>File system access</td>
<td>Store/retrieve test results, logs, calibration files</td>
</tr>
<tr>
<td><strong>@modelcontextprotocol/server-postgres</strong></td>
<td>Database operations</td>
<td>Store test history, instrument configs, user preferences</td>
</tr>
<tr>
<td><strong>@modelcontextprotocol/server-slack</strong></td>
<td>Notifications</td>
<td>Alert engineers when tests fail or anomalies detected</td>
</tr>
</tbody>
</table>
<h4><strong>Tier 2: Enhanced Features (Medium Priority)</strong></h4>
<table>
<thead>
<tr>
<th>MCP Server</th>
<th>Purpose</th>
<th>Use Case in EnnoiaCAT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>@modelcontextprotocol/server-google-drive</strong></td>
<td>Cloud storage</td>
<td>Automatic backup of test reports</td>
</tr>
<tr>
<td><strong>@modelcontextprotocol/server-git</strong></td>
<td>Version control</td>
<td>Track test configurations over time</td>
</tr>
<tr>
<td><strong>Custom: Spectrum ML Server</strong></td>
<td>Advanced ML analysis</td>
<td>Deep learning-based spectrum anomaly detection</td>
</tr>
</tbody>
</table>
<h4><strong>Tier 3: Advanced (Low Priority)</strong></h4>
<table>
<thead>
<tr>
<th>MCP Server</th>
<th>Purpose</th>
<th>Use Case in EnnoiaCAT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Custom: Multi-Site Orchestration</strong></td>
<td>Distributed testing</td>
<td>Coordinate tests across multiple labs</td>
</tr>
<tr>
<td><strong>Custom: Compliance Server</strong></td>
<td>Regulatory compliance</td>
<td>Validate tests against FCC/ETSI/3GPP standards</td>
</tr>
<tr>
<td><strong>@modelcontextprotocol/server-azure</strong></td>
<td>Azure integration</td>
<td>Deploy agents to Azure Functions</td>
</tr>
</tbody>
</table>
<h3>5.4 MCP Implementation Guide</h3>
<h4><strong>Step 1: Install MCP Client Library</strong></h4>
<pre><code class="language-bash">pip install mcp-client
</code></pre>
<h4><strong>Step 2: Create MCP Client Manager</strong></h4>
<pre><code class="language-python"># mcp_manager.py
from mcp.client import ClientSession, StdioServerParameters
from mcp import types
import asyncio

class MCPClientManager:
    def __init__(self):
        self.sessions = {}  # server_name -&gt; ClientSession

    async def connect_server(self, server_name: str, command: str, args: list):
        &quot;&quot;&quot;Connect to an MCP server via stdio&quot;&quot;&quot;
        server_params = StdioServerParameters(
            command=command,
            args=args,
            env=None
        )

        session = ClientSession(server_params)
        await session.initialize()

        self.sessions[server_name] = session
        return session

    async def list_tools(self, server_name: str) -&gt; list:
        &quot;&quot;&quot;List available tools from a server&quot;&quot;&quot;
        session = self.sessions.get(server_name)
        if not session:
            raise ValueError(f&quot;Not connected to {server_name}&quot;)

        result = await session.list_tools()
        return result.tools

    async def call_tool(self, server_name: str, tool_name: str, arguments: dict):
        &quot;&quot;&quot;Call a tool on an MCP server&quot;&quot;&quot;
        session = self.sessions.get(server_name)
        if not session:
            raise ValueError(f&quot;Not connected to {server_name}&quot;)

        result = await session.call_tool(tool_name, arguments)
        return result
</code></pre>
<h4><strong>Step 3: Integrate into Agent Base Class</strong></h4>
<pre><code class="language-python"># base_agent.py
from abc import ABC, abstractmethod
from mcp_manager import MCPClientManager

class BaseAgent(ABC):
    def __init__(self, name: str, mcp_manager: MCPClientManager = None):
        self.name = name
        self.mcp_manager = mcp_manager or MCPClientManager()
        self.capabilities = []

    @abstractmethod
    async def execute_task(self, task: dict):
        &quot;&quot;&quot;Execute a task assigned to this agent&quot;&quot;&quot;
        pass

    async def use_mcp_tool(self, server_name: str, tool_name: str, **kwargs):
        &quot;&quot;&quot;Convenience method for calling MCP tools&quot;&quot;&quot;
        return await self.mcp_manager.call_tool(server_name, tool_name, kwargs)
</code></pre>
<h4><strong>Step 4: Example MCP-Enhanced Agent</strong></h4>
<pre><code class="language-python"># storage_agent.py
class StorageAgent(BaseAgent):
    async def save_test_results(self, test_id: str, results: dict):
        &quot;&quot;&quot;Save test results using MCP filesystem server&quot;&quot;&quot;

        # Format results as JSON
        import json
        data = json.dumps(results, indent=2)

        # Save via MCP filesystem server
        await self.use_mcp_tool(
            server_name=&quot;filesystem&quot;,
            tool_name=&quot;write_file&quot;,
            path=f&quot;./test_results/{test_id}.json&quot;,
            content=data
        )

        # Also notify via Slack (if available)
        try:
            await self.use_mcp_tool(
                server_name=&quot;slack&quot;,
                tool_name=&quot;post_message&quot;,
                channel=&quot;#test-results&quot;,
                text=f&quot;Test {test_id} completed. Results saved.&quot;
            )
        except Exception:
            # Gracefully handle if Slack MCP not available
            pass
</code></pre>
<h3>5.5 MCP Configuration File</h3>
<p>Create <code>mcp_config.json</code> for easy server management:</p>
<pre><code class="language-json">{
  &quot;mcpServers&quot;: {
    &quot;filesystem&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;./&quot;]
    },
    &quot;postgres&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-postgres&quot;, &quot;postgresql://user:pass@localhost/ennoiacat&quot;]
    },
    &quot;slack&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-slack&quot;],
      &quot;env&quot;: {
        &quot;SLACK_BOT_TOKEN&quot;: &quot;${SLACK_BOT_TOKEN}&quot;,
        &quot;SLACK_TEAM_ID&quot;: &quot;${SLACK_TEAM_ID}&quot;
      }
    },
    &quot;spectrum-ml&quot;: {
      &quot;command&quot;: &quot;python&quot;,
      &quot;args&quot;: [&quot;./custom_mcp_servers/spectrum_ml_server.py&quot;]
    }
  }
}
</code></pre>
<hr />
<h2>6. Step-by-Step Execution Plan</h2>
<h3><strong>PHASE 0: Preparation &amp; Planning (1 week)</strong></h3>
<h4><strong>Week 1: Foundation Setup</strong></h4>
<p><strong>Day 1-2: Code Audit &amp; Documentation</strong>
- [ ] Document all existing agents and their interfaces
- [ ] Map out current workflows (Configuration → Location → Analysis)
- [ ] Identify code dependencies and potential breaking points
- [ ] Create baseline performance benchmarks</p>
<p><strong>Day 3-4: Development Environment Setup</strong>
- [ ] Set up feature branch: <code>feature/agentic-framework</code>
- [ ] Configure additional testing environments
- [ ] Install MCP client libraries: <code>pip install mcp-client</code>
- [ ] Set up example MCP servers for testing</p>
<p><strong>Day 5-7: Design Validation</strong>
- [ ] Review this document with stakeholders
- [ ] Finalize agent taxonomy and responsibilities
- [ ] Create detailed UML diagrams for new architecture
- [ ] Define success metrics and KPIs</p>
<p><strong>Deliverables:</strong>
- Architecture design document (this document)
- UML diagrams
- Test plan document
- Git feature branch ready</p>
<hr />
<h3><strong>PHASE 1: Agent Foundation (2 weeks)</strong></h3>
<h4><strong>Week 1-2: Base Agent Infrastructure</strong></h4>
<p><strong>Task 1.1: Create Base Agent Class</strong> <em>(2 days)</em></p>
<pre><code class="language-python"># agents/base_agent.py
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import asyncio

class AgentStatus(Enum):
    IDLE = &quot;idle&quot;
    BUSY = &quot;busy&quot;
    ERROR = &quot;error&quot;
    OFFLINE = &quot;offline&quot;

@dataclass
class AgentCapability:
    name: str
    description: str
    input_schema: dict
    output_schema: dict

@dataclass
class Task:
    task_id: str
    task_type: str
    parameters: dict
    priority: int = 0
    requester: str = &quot;user&quot;

class BaseAgent(ABC):
    def __init__(self, agent_id: str, name: str, description: str):
        self.agent_id = agent_id
        self.name = name
        self.description = description
        self.status = AgentStatus.IDLE
        self.capabilities: List[AgentCapability] = []
        self.task_queue: asyncio.Queue = asyncio.Queue()
        self.message_handlers = {}

    @abstractmethod
    async def execute_task(self, task: Task) -&gt; Any:
        &quot;&quot;&quot;Execute a task assigned to this agent&quot;&quot;&quot;
        pass

    @abstractmethod
    def get_capabilities(self) -&gt; List[AgentCapability]:
        &quot;&quot;&quot;Return list of capabilities this agent provides&quot;&quot;&quot;
        pass

    async def send_message(self, target_agent: str, message: dict):
        &quot;&quot;&quot;Send message to another agent via message bus&quot;&quot;&quot;
        await self.message_bus.publish(target_agent, message)

    async def start(self):
        &quot;&quot;&quot;Start the agent's event loop&quot;&quot;&quot;
        while True:
            task = await self.task_queue.get()
            try:
                self.status = AgentStatus.BUSY
                result = await self.execute_task(task)
                await self.report_completion(task.task_id, result)
            except Exception as e:
                self.status = AgentStatus.ERROR
                await self.report_error(task.task_id, str(e))
            finally:
                self.status = AgentStatus.IDLE
</code></pre>
<p><strong>Task 1.2: Refactor Existing Agents</strong> <em>(3 days)</em></p>
<ul>
<li>[ ] Refactor <code>ConfigurationAgent</code> to extend <code>BaseAgent</code></li>
<li>[ ] Refactor <code>LocationAgent</code> to extend <code>BaseAgent</code></li>
<li>[ ] Refactor <code>AnalysisAgent</code> to extend <code>BaseAgent</code></li>
<li>[ ] Add capability definitions for each agent</li>
<li>[ ] Write unit tests for each refactored agent</li>
</ul>
<p><strong>Example Refactoring:</strong></p>
<pre><code class="language-python"># agents/configuration_agent.py
from agents.base_agent import BaseAgent, AgentCapability, Task
from tinySA_config import TinySAHelper
from map_api import MapAPI

class ConfigurationAgent(BaseAgent):
    def __init__(self):
        super().__init__(
            agent_id=&quot;config-001&quot;,
            name=&quot;Configuration Agent&quot;,
            description=&quot;Handles model selection and device configuration&quot;
        )
        self.helper = TinySAHelper()
        self.provider = None

    def get_capabilities(self) -&gt; List[AgentCapability]:
        return [
            AgentCapability(
                name=&quot;select_ai_mode&quot;,
                description=&quot;Choose between SLM (offline) and LLM (online) modes&quot;,
                input_schema={&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;mode&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;SLM&quot;, &quot;LLM&quot;]}}},
                output_schema={&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;model_loaded&quot;: {&quot;type&quot;: &quot;boolean&quot;}}}
            ),
            AgentCapability(
                name=&quot;detect_instruments&quot;,
                description=&quot;Detect available test instruments&quot;,
                input_schema={},
                output_schema={&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;object&quot;}}
            )
        ]

    async def execute_task(self, task: Task) -&gt; Any:
        if task.task_type == &quot;select_ai_mode&quot;:
            return await self._select_ai_mode(task.parameters[&quot;mode&quot;])
        elif task.task_type == &quot;detect_instruments&quot;:
            return await self._detect_instruments()
        else:
            raise ValueError(f&quot;Unknown task type: {task.task_type}&quot;)
</code></pre>
<p><strong>Task 1.3: Create Agent Registry</strong> <em>(2 days)</em></p>
<pre><code class="language-python"># agents/agent_registry.py
from typing import Dict, List, Optional
from agents.base_agent import BaseAgent, AgentCapability
import threading

class AgentRegistry:
    &quot;&quot;&quot;Singleton registry for managing all agents&quot;&quot;&quot;

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._agents = {}
                    cls._instance._capabilities_index = {}
        return cls._instance

    def register_agent(self, agent: BaseAgent):
        &quot;&quot;&quot;Register a new agent&quot;&quot;&quot;
        self._agents[agent.agent_id] = agent

        # Index capabilities for discovery
        for capability in agent.get_capabilities():
            if capability.name not in self._capabilities_index:
                self._capabilities_index[capability.name] = []
            self._capabilities_index[capability.name].append(agent.agent_id)

    def unregister_agent(self, agent_id: str):
        &quot;&quot;&quot;Remove an agent from registry&quot;&quot;&quot;
        if agent_id in self._agents:
            del self._agents[agent_id]

    def get_agent(self, agent_id: str) -&gt; Optional[BaseAgent]:
        &quot;&quot;&quot;Get agent by ID&quot;&quot;&quot;
        return self._agents.get(agent_id)

    def find_agents_by_capability(self, capability_name: str) -&gt; List[BaseAgent]:
        &quot;&quot;&quot;Find all agents that provide a specific capability&quot;&quot;&quot;
        agent_ids = self._capabilities_index.get(capability_name, [])
        return [self._agents[aid] for aid in agent_ids if aid in self._agents]

    def list_all_agents(self) -&gt; List[BaseAgent]:
        &quot;&quot;&quot;Get all registered agents&quot;&quot;&quot;
        return list(self._agents.values())
</code></pre>
<p><strong>Task 1.4: Implement Message Bus</strong> <em>(2 days)</em></p>
<pre><code class="language-python"># agents/message_bus.py
import asyncio
from typing import Dict, Callable, Any
from collections import defaultdict

class MessageBus:
    &quot;&quot;&quot;Simple pub/sub message bus for agent communication&quot;&quot;&quot;

    def __init__(self):
        self.subscribers: Dict[str, List[Callable]] = defaultdict(list)
        self.message_queue = asyncio.Queue()

    def subscribe(self, topic: str, callback: Callable):
        &quot;&quot;&quot;Subscribe to a topic&quot;&quot;&quot;
        self.subscribers[topic].append(callback)

    def unsubscribe(self, topic: str, callback: Callable):
        &quot;&quot;&quot;Unsubscribe from a topic&quot;&quot;&quot;
        if callback in self.subscribers[topic]:
            self.subscribers[topic].remove(callback)

    async def publish(self, topic: str, message: Any):
        &quot;&quot;&quot;Publish a message to a topic&quot;&quot;&quot;
        for callback in self.subscribers[topic]:
            await callback(message)

    async def request_response(self, topic: str, message: Any, timeout: float = 30.0):
        &quot;&quot;&quot;Publish and wait for single response&quot;&quot;&quot;
        response_future = asyncio.Future()

        def response_handler(msg):
            if not response_future.done():
                response_future.set_result(msg)

        self.subscribe(f&quot;{topic}_response&quot;, response_handler)
        await self.publish(topic, message)

        try:
            result = await asyncio.wait_for(response_future, timeout=timeout)
            return result
        finally:
            self.unsubscribe(f&quot;{topic}_response&quot;, response_handler)
</code></pre>
<p><strong>Deliverables:</strong>
- <code>agents/base_agent.py</code> - Base agent class
- <code>agents/configuration_agent.py</code> - Refactored configuration agent
- <code>agents/location_agent.py</code> - Refactored location agent
- <code>agents/analysis_agent.py</code> - Refactored analysis agent
- <code>agents/agent_registry.py</code> - Agent registry
- <code>agents/message_bus.py</code> - Message bus
- Unit tests for all components</p>
<hr />
<h3><strong>PHASE 2: Orchestration Layer (2-3 weeks)</strong></h3>
<h4><strong>Week 3-4: Master Orchestrator</strong></h4>
<p><strong>Task 2.1: Workflow Definition Language</strong> <em>(3 days)</em></p>
<p>Create a simple DSL for defining workflows:</p>
<pre><code class="language-python"># workflows/workflow_dsl.py
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from enum import Enum

class StepType(Enum):
    SEQUENTIAL = &quot;sequential&quot;  # Steps run one after another
    PARALLEL = &quot;parallel&quot;      # Steps run concurrently
    CONDITIONAL = &quot;conditional&quot; # Step runs based on condition

@dataclass
class WorkflowStep:
    step_id: str
    agent_capability: str  # Capability name to invoke
    parameters: Dict[str, Any]
    step_type: StepType = StepType.SEQUENTIAL
    condition: Optional[str] = None  # Python expression
    next_steps: List[str] = None

    def __post_init__(self):
        if self.next_steps is None:
            self.next_steps = []

@dataclass
class Workflow:
    workflow_id: str
    name: str
    description: str
    steps: List[WorkflowStep]
    entry_point: str  # ID of first step

# Example workflow: Standard spectrum analysis
standard_spectrum_analysis = Workflow(
    workflow_id=&quot;standard-spectrum-001&quot;,
    name=&quot;Standard Spectrum Analysis&quot;,
    description=&quot;Detect location, configure instrument, run analysis&quot;,
    entry_point=&quot;step1&quot;,
    steps=[
        WorkflowStep(
            step_id=&quot;step1&quot;,
            agent_capability=&quot;detect_location&quot;,
            parameters={},
            next_steps=[&quot;step2&quot;]
        ),
        WorkflowStep(
            step_id=&quot;step2&quot;,
            agent_capability=&quot;select_ai_mode&quot;,
            parameters={&quot;mode&quot;: &quot;SLM&quot;},
            next_steps=[&quot;step3&quot;]
        ),
        WorkflowStep(
            step_id=&quot;step3&quot;,
            agent_capability=&quot;run_spectrum_scan&quot;,
            parameters={&quot;instrument&quot;: &quot;TinySA&quot;},
            next_steps=[&quot;step4&quot;]
        ),
        WorkflowStep(
            step_id=&quot;step4&quot;,
            agent_capability=&quot;analyze_spectrum&quot;,
            parameters={&quot;method&quot;: &quot;anomaly_detection&quot;},
            next_steps=[]
        )
    ]
)
</code></pre>
<p><strong>Task 2.2: Implement Master Orchestrator</strong> <em>(4 days)</em></p>
<pre><code class="language-python"># agents/orchestrator_agent.py
from agents.base_agent import BaseAgent, Task
from workflows.workflow_dsl import Workflow, StepType
from agents.agent_registry import AgentRegistry
import asyncio

class MasterOrchestrator(BaseAgent):
    def __init__(self, agent_registry: AgentRegistry, message_bus):
        super().__init__(
            agent_id=&quot;orchestrator-001&quot;,
            name=&quot;Master Orchestrator&quot;,
            description=&quot;Coordinates multi-agent workflows&quot;
        )
        self.registry = agent_registry
        self.message_bus = message_bus
        self.active_workflows = {}

    async def execute_workflow(self, workflow: Workflow, context: dict = None):
        &quot;&quot;&quot;Execute a complete workflow&quot;&quot;&quot;
        context = context or {}

        # Start from entry point
        current_step_ids = [workflow.entry_point]

        while current_step_ids:
            # Get current steps
            current_steps = [
                step for step in workflow.steps
                if step.step_id in current_step_ids
            ]

            # Separate sequential and parallel steps
            sequential_steps = [s for s in current_steps if s.step_type == StepType.SEQUENTIAL]
            parallel_steps = [s for s in current_steps if s.step_type == StepType.PARALLEL]

            # Execute sequential steps
            next_step_ids = []
            for step in sequential_steps:
                if self._evaluate_condition(step.condition, context):
                    result = await self._execute_step(step, context)
                    context[step.step_id] = result
                    next_step_ids.extend(step.next_steps)

            # Execute parallel steps concurrently
            if parallel_steps:
                tasks = [
                    self._execute_step(step, context)
                    for step in parallel_steps
                    if self._evaluate_condition(step.condition, context)
                ]
                results = await asyncio.gather(*tasks)

                for step, result in zip(parallel_steps, results):
                    context[step.step_id] = result
                    next_step_ids.extend(step.next_steps)

            current_step_ids = next_step_ids

        return context

    async def _execute_step(self, step: WorkflowStep, context: dict):
        &quot;&quot;&quot;Execute a single workflow step&quot;&quot;&quot;
        # Find agent with required capability
        agents = self.registry.find_agents_by_capability(step.agent_capability)

        if not agents:
            raise ValueError(f&quot;No agent found with capability: {step.agent_capability}&quot;)

        # Use first available agent (could implement load balancing here)
        agent = agents[0]

        # Resolve parameters from context
        resolved_params = self._resolve_parameters(step.parameters, context)

        # Create task
        task = Task(
            task_id=f&quot;{step.step_id}-{asyncio.current_task().get_name()}&quot;,
            task_type=step.agent_capability,
            parameters=resolved_params,
            requester=&quot;orchestrator&quot;
        )

        # Execute task
        await agent.task_queue.put(task)

        # Wait for result (simplified - in production use proper async communication)
        result = await self._wait_for_result(task.task_id)
        return result

    def _evaluate_condition(self, condition: Optional[str], context: dict) -&gt; bool:
        &quot;&quot;&quot;Evaluate a condition expression&quot;&quot;&quot;
        if condition is None:
            return True
        try:
            return eval(condition, {&quot;context&quot;: context})
        except Exception:
            return False

    def _resolve_parameters(self, params: dict, context: dict) -&gt; dict:
        &quot;&quot;&quot;Resolve parameter references from context&quot;&quot;&quot;
        resolved = {}
        for key, value in params.items():
            if isinstance(value, str) and value.startswith(&quot;$&quot;):
                # Reference to context variable
                context_key = value[1:]
                resolved[key] = context.get(context_key, value)
            else:
                resolved[key] = value
        return resolved
</code></pre>
<p><strong>Task 2.3: Natural Language to Workflow Translation</strong> <em>(3 days)</em></p>
<p>Use LLM to translate user requests into workflows:</p>
<pre><code class="language-python"># agents/workflow_translator.py
from typing import Dict, Any
from workflows.workflow_dsl import Workflow, WorkflowStep, StepType
import json

class WorkflowTranslator:
    &quot;&quot;&quot;Translates natural language to workflow definitions using LLM&quot;&quot;&quot;

    def __init__(self, llm_client):
        self.llm_client = llm_client

    async def translate(self, user_request: str) -&gt; Workflow:
        &quot;&quot;&quot;Convert natural language request to workflow&quot;&quot;&quot;

        system_prompt = &quot;&quot;&quot;You are a workflow generation expert for a test instrument platform.

Available agent capabilities:
- detect_location: Detect geographic location
- select_ai_mode: Choose AI mode (SLM or LLM)
- detect_instruments: Find connected instruments
- configure_instrument: Set up instrument parameters
- run_spectrum_scan: Execute spectrum scan
- analyze_spectrum: Analyze spectrum data
- generate_report: Create test report

Generate a workflow in JSON format that accomplishes the user's request.

Example output:
{
  &quot;workflow_id&quot;: &quot;custom-001&quot;,
  &quot;name&quot;: &quot;User Custom Workflow&quot;,
  &quot;description&quot;: &quot;Generated from user request&quot;,
  &quot;entry_point&quot;: &quot;step1&quot;,
  &quot;steps&quot;: [
    {
      &quot;step_id&quot;: &quot;step1&quot;,
      &quot;agent_capability&quot;: &quot;detect_instruments&quot;,
      &quot;parameters&quot;: {},
      &quot;step_type&quot;: &quot;sequential&quot;,
      &quot;next_steps&quot;: [&quot;step2&quot;]
    }
  ]
}
&quot;&quot;&quot;

        response = await self.llm_client.chat.completions.create(
            model=&quot;gpt-4&quot;,
            messages=[
                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt},
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Create workflow for: {user_request}&quot;}
            ]
        )

        workflow_json = json.loads(response.choices[0].message.content)

        # Convert JSON to Workflow object
        steps = [
            WorkflowStep(
                step_id=s[&quot;step_id&quot;],
                agent_capability=s[&quot;agent_capability&quot;],
                parameters=s[&quot;parameters&quot;],
                step_type=StepType(s.get(&quot;step_type&quot;, &quot;sequential&quot;)),
                next_steps=s.get(&quot;next_steps&quot;, [])
            )
            for s in workflow_json[&quot;steps&quot;]
        ]

        return Workflow(
            workflow_id=workflow_json[&quot;workflow_id&quot;],
            name=workflow_json[&quot;name&quot;],
            description=workflow_json[&quot;description&quot;],
            entry_point=workflow_json[&quot;entry_point&quot;],
            steps=steps
        )
</code></pre>
<p><strong>Deliverables:</strong>
- <code>workflows/workflow_dsl.py</code> - Workflow definition language
- <code>workflows/standard_workflows.py</code> - Pre-defined workflows
- <code>agents/orchestrator_agent.py</code> - Master orchestrator implementation
- <code>agents/workflow_translator.py</code> - Natural language translator
- Integration tests for workflow execution</p>
<hr />
<h3><strong>PHASE 3: MCP Integration (2-3 weeks)</strong></h3>
<h4><strong>Week 5-6: MCP Client Implementation</strong></h4>
<p><strong>Task 3.1: Install MCP Dependencies</strong> <em>(1 day)</em></p>
<pre><code class="language-bash"># Install MCP Python client
pip install mcp

# Install Node.js MCP servers (examples)
npm install -g @modelcontextprotocol/server-filesystem
npm install -g @modelcontextprotocol/server-postgres
npm install -g @modelcontextprotocol/server-slack
</code></pre>
<p><strong>Task 3.2: MCP Client Manager</strong> <em>(3 days)</em></p>
<p>Already outlined in section 5.4, implement:
- Connection management
- Tool discovery
- Resource access
- Error handling and reconnection logic</p>
<p><strong>Task 3.3: Create Custom MCP Server for Spectrum Analysis</strong> <em>(4 days)</em></p>
<p>Example custom MCP server:</p>
<pre><code class="language-python"># custom_mcp_servers/spectrum_ml_server.py
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types
import numpy as np
import json

# Your ML model for spectrum analysis
from your_ml_models import SpectrumAnomalyDetector

server = Server(&quot;spectrum-ml-server&quot;)

# Initialize ML model
detector = SpectrumAnomalyDetector()

@server.list_tools()
async def handle_list_tools() -&gt; list[types.Tool]:
    &quot;&quot;&quot;List available tools&quot;&quot;&quot;
    return [
        types.Tool(
            name=&quot;advanced_anomaly_detection&quot;,
            description=&quot;Detect anomalies in spectrum data using deep learning&quot;,
            inputSchema={
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;spectrum_data&quot;: {
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;items&quot;: {&quot;type&quot;: &quot;number&quot;},
                        &quot;description&quot;: &quot;Array of spectrum power values&quot;
                    },
                    &quot;frequencies&quot;: {
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;items&quot;: {&quot;type&quot;: &quot;number&quot;},
                        &quot;description&quot;: &quot;Corresponding frequency values&quot;
                    },
                    &quot;threshold&quot;: {
                        &quot;type&quot;: &quot;number&quot;,
                        &quot;description&quot;: &quot;Anomaly detection threshold (0-1)&quot;,
                        &quot;default&quot;: 0.8
                    }
                },
                &quot;required&quot;: [&quot;spectrum_data&quot;, &quot;frequencies&quot;]
            }
        ),
        types.Tool(
            name=&quot;classify_signal_type&quot;,
            description=&quot;Classify signal modulation type&quot;,
            inputSchema={
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;iq_data&quot;: {
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;description&quot;: &quot;IQ sample data&quot;
                    }
                },
                &quot;required&quot;: [&quot;iq_data&quot;]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(
    name: str, arguments: dict
) -&gt; list[types.TextContent]:
    &quot;&quot;&quot;Handle tool calls&quot;&quot;&quot;

    if name == &quot;advanced_anomaly_detection&quot;:
        spectrum = np.array(arguments[&quot;spectrum_data&quot;])
        frequencies = np.array(arguments[&quot;frequencies&quot;])
        threshold = arguments.get(&quot;threshold&quot;, 0.8)

        # Run ML model
        anomalies = detector.detect(spectrum, frequencies, threshold)

        result = {
            &quot;anomalies_detected&quot;: len(anomalies),
            &quot;anomaly_indices&quot;: [int(a) for a in anomalies],
            &quot;anomaly_frequencies&quot;: [float(frequencies[a]) for a in anomalies],
            &quot;anomaly_powers&quot;: [float(spectrum[a]) for a in anomalies]
        }

        return [types.TextContent(
            type=&quot;text&quot;,
            text=json.dumps(result)
        )]

    elif name == &quot;classify_signal_type&quot;:
        # Implement signal classification
        pass

    else:
        raise ValueError(f&quot;Unknown tool: {name}&quot;)

async def main():
    # Run the server using stdin/stdout streams
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name=&quot;spectrum-ml-server&quot;,
                server_version=&quot;0.1.0&quot;,
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                )
            )
        )

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
</code></pre>
<p><strong>Task 3.4: Integrate MCP into Agents</strong> <em>(3 days)</em></p>
<p>Update base agent to support MCP:</p>
<pre><code class="language-python"># agents/base_agent.py (updated)
from mcp_manager import MCPClientManager

class BaseAgent(ABC):
    def __init__(self, agent_id: str, name: str, description: str,
                 mcp_manager: MCPClientManager = None):
        # ... existing code ...
        self.mcp_manager = mcp_manager
        self.mcp_tools_cache = {}

    async def discover_mcp_tools(self):
        &quot;&quot;&quot;Discover available MCP tools from all connected servers&quot;&quot;&quot;
        if not self.mcp_manager:
            return

        for server_name in self.mcp_manager.sessions.keys():
            tools = await self.mcp_manager.list_tools(server_name)
            self.mcp_tools_cache[server_name] = tools

    async def use_mcp_tool(self, server_name: str, tool_name: str, **kwargs):
        &quot;&quot;&quot;Call an MCP tool with fallback handling&quot;&quot;&quot;
        try:
            result = await self.mcp_manager.call_tool(server_name, tool_name, kwargs)
            return result
        except Exception as e:
            # Log error and potentially fallback to local implementation
            self.log_error(f&quot;MCP tool call failed: {e}&quot;)
            return await self.fallback_handler(tool_name, **kwargs)

    async def fallback_handler(self, tool_name: str, **kwargs):
        &quot;&quot;&quot;Override in subclass to provide fallback behavior&quot;&quot;&quot;
        raise NotImplementedError(&quot;MCP tool unavailable and no fallback defined&quot;)
</code></pre>
<p><strong>Task 3.5: Configuration and Discovery</strong> <em>(2 days)</em></p>
<p>Create MCP configuration system:</p>
<pre><code class="language-python"># mcp_config_loader.py
import json
import os
from mcp_manager import MCPClientManager

class MCPConfigLoader:
    def __init__(self, config_path: str = &quot;mcp_config.json&quot;):
        self.config_path = config_path
        self.mcp_manager = MCPClientManager()

    async def load_and_connect(self):
        &quot;&quot;&quot;Load config and connect to all MCP servers&quot;&quot;&quot;
        with open(self.config_path, 'r') as f:
            config = json.load(f)

        servers = config.get(&quot;mcpServers&quot;, {})

        for server_name, server_config in servers.items():
            try:
                # Resolve environment variables in config
                env = server_config.get(&quot;env&quot;, {})
                resolved_env = {
                    k: os.environ.get(v.strip(&quot;${}&quot;), v)
                    for k, v in env.items()
                }

                await self.mcp_manager.connect_server(
                    server_name=server_name,
                    command=server_config[&quot;command&quot;],
                    args=server_config[&quot;args&quot;],
                    env=resolved_env
                )
                print(f&quot;✓ Connected to MCP server: {server_name}&quot;)
            except Exception as e:
                print(f&quot;✗ Failed to connect to {server_name}: {e}&quot;)

        return self.mcp_manager
</code></pre>
<p><strong>Deliverables:</strong>
- <code>mcp_manager.py</code> - MCP client manager
- <code>mcp_config_loader.py</code> - Configuration loader
- <code>custom_mcp_servers/spectrum_ml_server.py</code> - Custom MCP server
- <code>mcp_config.json</code> - MCP configuration file
- Updated <code>agents/base_agent.py</code> with MCP support
- Integration tests with real MCP servers</p>
<hr />
<h3><strong>PHASE 4: Advanced Features (2-3 weeks)</strong></h3>
<h4><strong>Week 7-8: Agent Memory &amp; Learning</strong></h4>
<p><strong>Task 4.1: Implement Agent Memory System</strong> <em>(4 days)</em></p>
<pre><code class="language-python"># agents/memory/agent_memory.py
from typing import List, Dict, Any
import chromadb
from datetime import datetime

class AgentMemory:
    &quot;&quot;&quot;RAG-based memory system for agents&quot;&quot;&quot;

    def __init__(self, agent_id: str, persist_directory: str = &quot;./agent_memory&quot;):
        self.agent_id = agent_id
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name=f&quot;agent_{agent_id}_memory&quot;
        )

    async def store_experience(self, experience_type: str, context: dict,
                               outcome: dict, success: bool):
        &quot;&quot;&quot;Store an experience in memory&quot;&quot;&quot;
        document = {
            &quot;type&quot;: experience_type,
            &quot;context&quot;: context,
            &quot;outcome&quot;: outcome,
            &quot;success&quot;: success,
            &quot;timestamp&quot;: datetime.now().isoformat()
        }

        # Create embedding-friendly text
        text = f&quot;{experience_type}: {str(context)} -&gt; {str(outcome)}&quot;

        self.collection.add(
            documents=[text],
            metadatas=[document],
            ids=[f&quot;{self.agent_id}_{datetime.now().timestamp()}&quot;]
        )

    async def recall_similar_experiences(self, current_context: str, n: int = 5):
        &quot;&quot;&quot;Retrieve similar past experiences&quot;&quot;&quot;
        results = self.collection.query(
            query_texts=[current_context],
            n_results=n
        )

        return results[&quot;metadatas&quot;][0] if results[&quot;metadatas&quot;] else []

    async def learn_from_failure(self, failed_task: Dict, error: str):
        &quot;&quot;&quot;Store failure for future learning&quot;&quot;&quot;
        await self.store_experience(
            experience_type=&quot;failure&quot;,
            context=failed_task,
            outcome={&quot;error&quot;: error},
            success=False
        )
</code></pre>
<p><strong>Task 4.2: Adaptive Agent Behavior</strong> <em>(3 days)</em></p>
<pre><code class="language-python"># agents/adaptive_agent.py
from agents.base_agent import BaseAgent
from agents.memory.agent_memory import AgentMemory

class AdaptiveAgent(BaseAgent):
    &quot;&quot;&quot;Base class for agents with learning capabilities&quot;&quot;&quot;

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.memory = AgentMemory(self.agent_id)

    async def execute_task_with_learning(self, task: Task):
        &quot;&quot;&quot;Execute task and learn from outcome&quot;&quot;&quot;

        # Recall similar past experiences
        context_str = f&quot;{task.task_type}: {str(task.parameters)}&quot;
        past_experiences = await self.memory.recall_similar_experiences(context_str)

        # Use past experiences to inform execution
        if past_experiences:
            successful_experiences = [e for e in past_experiences if e[&quot;success&quot;]]
            if successful_experiences:
                # Apply learned optimizations
                task = self._apply_learned_optimizations(task, successful_experiences)

        # Execute task
        try:
            result = await self.execute_task(task)

            # Store successful experience
            await self.memory.store_experience(
                experience_type=task.task_type,
                context=task.parameters,
                outcome=result,
                success=True
            )

            return result

        except Exception as e:
            # Learn from failure
            await self.memory.learn_from_failure(
                failed_task={&quot;type&quot;: task.task_type, &quot;params&quot;: task.parameters},
                error=str(e)
            )
            raise

    def _apply_learned_optimizations(self, task: Task, experiences: List[Dict]):
        &quot;&quot;&quot;Apply optimizations learned from past successful experiences&quot;&quot;&quot;
        # Example: adjust parameters based on what worked before
        for exp in experiences:
            if &quot;optimization&quot; in exp[&quot;outcome&quot;]:
                task.parameters.update(exp[&quot;outcome&quot;][&quot;optimization&quot;])
        return task
</code></pre>
<p><strong>Task 4.3: Performance Monitoring &amp; Auto-Tuning</strong> <em>(3 days)</em></p>
<pre><code class="language-python"># agents/performance_monitor.py
import time
import statistics
from collections import defaultdict
from typing import Dict, List

class PerformanceMonitor:
    &quot;&quot;&quot;Monitors agent performance and suggests optimizations&quot;&quot;&quot;

    def __init__(self):
        self.execution_times: Dict[str, List[float]] = defaultdict(list)
        self.error_counts: Dict[str, int] = defaultdict(int)
        self.success_counts: Dict[str, int] = defaultdict(int)

    def record_execution(self, agent_id: str, task_type: str,
                        duration: float, success: bool):
        &quot;&quot;&quot;Record a task execution&quot;&quot;&quot;
        key = f&quot;{agent_id}:{task_type}&quot;
        self.execution_times[key].append(duration)

        if success:
            self.success_counts[key] += 1
        else:
            self.error_counts[key] += 1

    def get_performance_report(self, agent_id: str = None) -&gt; Dict:
        &quot;&quot;&quot;Generate performance report&quot;&quot;&quot;
        report = {}

        for key in self.execution_times.keys():
            if agent_id and not key.startswith(agent_id):
                continue

            times = self.execution_times[key]
            total_executions = self.success_counts[key] + self.error_counts[key]

            report[key] = {
                &quot;avg_duration&quot;: statistics.mean(times),
                &quot;median_duration&quot;: statistics.median(times),
                &quot;std_dev&quot;: statistics.stdev(times) if len(times) &gt; 1 else 0,
                &quot;success_rate&quot;: self.success_counts[key] / total_executions if total_executions &gt; 0 else 0,
                &quot;total_executions&quot;: total_executions
            }

        return report

    def suggest_optimizations(self, agent_id: str) -&gt; List[str]:
        &quot;&quot;&quot;Suggest performance optimizations&quot;&quot;&quot;
        suggestions = []
        report = self.get_performance_report(agent_id)

        for key, metrics in report.items():
            # High latency detection
            if metrics[&quot;avg_duration&quot;] &gt; 10.0:  # &gt; 10 seconds
                suggestions.append(
                    f&quot;{key}: Consider caching or parallel execution (avg: {metrics['avg_duration']:.2f}s)&quot;
                )

            # Low success rate detection
            if metrics[&quot;success_rate&quot;] &lt; 0.8:
                suggestions.append(
                    f&quot;{key}: Low success rate ({metrics['success_rate']:.2%}). Review error handling.&quot;
                )

            # High variance detection
            if metrics[&quot;std_dev&quot;] &gt; metrics[&quot;avg_duration&quot;] * 0.5:
                suggestions.append(
                    f&quot;{key}: High variance in execution time. Investigate bottlenecks.&quot;
                )

        return suggestions
</code></pre>
<p><strong>Deliverables:</strong>
- <code>agents/memory/agent_memory.py</code> - Memory system
- <code>agents/adaptive_agent.py</code> - Adaptive agent base class
- <code>agents/performance_monitor.py</code> - Performance monitoring
- Example of adaptive behavior in one agent
- Tests for memory and learning systems</p>
<hr />
<h3><strong>PHASE 5: UI Integration &amp; Polish (1-2 weeks)</strong></h3>
<h4><strong>Week 9-10: Streamlit UI Updates</strong></h4>
<p><strong>Task 5.1: Agentic Dashboard</strong> <em>(3 days)</em></p>
<pre><code class="language-python"># ui/agentic_dashboard.py
import streamlit as st
from agents.agent_registry import AgentRegistry
from agents.performance_monitor import PerformanceMonitor

def render_agentic_dashboard():
    &quot;&quot;&quot;Render agent monitoring dashboard&quot;&quot;&quot;

    st.header(&quot;🤖 Agentic Framework Dashboard&quot;)

    registry = AgentRegistry()
    monitor = st.session_state.get(&quot;performance_monitor&quot;, PerformanceMonitor())

    # Agent Status Overview
    st.subheader(&quot;Agent Status&quot;)
    agents = registry.list_all_agents()

    cols = st.columns(4)
    status_counts = {&quot;IDLE&quot;: 0, &quot;BUSY&quot;: 0, &quot;ERROR&quot;: 0, &quot;OFFLINE&quot;: 0}

    for agent in agents:
        status_counts[agent.status.value.upper()] += 1

    cols[0].metric(&quot;Total Agents&quot;, len(agents))
    cols[1].metric(&quot;Active&quot;, status_counts[&quot;BUSY&quot;])
    cols[2].metric(&quot;Idle&quot;, status_counts[&quot;IDLE&quot;])
    cols[3].metric(&quot;Errors&quot;, status_counts[&quot;ERROR&quot;])

    # Agent Details Table
    agent_data = []
    for agent in agents:
        capabilities = &quot;, &quot;.join([c.name for c in agent.get_capabilities()])
        agent_data.append({
            &quot;Agent ID&quot;: agent.agent_id,
            &quot;Name&quot;: agent.name,
            &quot;Status&quot;: agent.status.value,
            &quot;Capabilities&quot;: capabilities
        })

    st.dataframe(agent_data, use_container_width=True)

    # Performance Metrics
    st.subheader(&quot;Performance Metrics&quot;)
    perf_report = monitor.get_performance_report()

    if perf_report:
        for key, metrics in perf_report.items():
            with st.expander(f&quot;📊 {key}&quot;):
                col1, col2, col3 = st.columns(3)
                col1.metric(&quot;Avg Duration&quot;, f&quot;{metrics['avg_duration']:.2f}s&quot;)
                col2.metric(&quot;Success Rate&quot;, f&quot;{metrics['success_rate']:.1%}&quot;)
                col3.metric(&quot;Total Runs&quot;, metrics[&quot;total_executions&quot;])

    # Optimization Suggestions
    st.subheader(&quot;💡 Optimization Suggestions&quot;)
    suggestions = []
    for agent in agents:
        suggestions.extend(monitor.suggest_optimizations(agent.agent_id))

    if suggestions:
        for suggestion in suggestions:
            st.info(suggestion)
    else:
        st.success(&quot;No optimization suggestions at this time.&quot;)
</code></pre>
<p><strong>Task 5.2: Natural Language Workflow Creation</strong> <em>(3 days)</em></p>
<pre><code class="language-python"># ui/workflow_creator.py
import streamlit as st
from agents.workflow_translator import WorkflowTranslator
from agents.orchestrator_agent import MasterOrchestrator

def render_workflow_creator():
    &quot;&quot;&quot;UI for creating workflows via natural language&quot;&quot;&quot;

    st.header(&quot;🎯 Create Custom Workflow&quot;)

    st.markdown(&quot;&quot;&quot;
    Describe what you want to test in natural language. The AI will create
    an automated workflow for you.

    **Examples:**
    - &quot;Test 5G NR signal on band n77 in New York and generate a report&quot;
    - &quot;Scan WiFi channels, detect strongest signal, and analyze interference&quot;
    - &quot;Configure TinySA for 2.4 GHz, run scan, find anomalies&quot;
    &quot;&quot;&quot;)

    user_request = st.text_area(
        &quot;What do you want to test?&quot;,
        height=100,
        placeholder=&quot;Describe your test workflow...&quot;
    )

    if st.button(&quot;🚀 Generate Workflow&quot;, type=&quot;primary&quot;):
        if not user_request:
            st.error(&quot;Please enter a workflow description&quot;)
            return

        with st.spinner(&quot;Generating workflow...&quot;):
            translator = WorkflowTranslator(st.session_state.llm_client)

            try:
                workflow = await translator.translate(user_request)

                # Display generated workflow
                st.success(&quot;✅ Workflow generated!&quot;)

                st.subheader(&quot;Generated Workflow&quot;)
                st.json({
                    &quot;name&quot;: workflow.name,
                    &quot;description&quot;: workflow.description,
                    &quot;steps&quot;: [
                        {
                            &quot;step&quot;: s.step_id,
                            &quot;action&quot;: s.agent_capability,
                            &quot;params&quot;: s.parameters
                        }
                        for s in workflow.steps
                    ]
                })

                # Store in session state
                st.session_state.generated_workflow = workflow

                # Execution button
                if st.button(&quot;▶️ Execute Workflow&quot;):
                    orchestrator = st.session_state.orchestrator

                    with st.spinner(&quot;Executing workflow...&quot;):
                        result = await orchestrator.execute_workflow(workflow)

                    st.success(&quot;✅ Workflow completed!&quot;)
                    st.json(result)

            except Exception as e:
                st.error(f&quot;Failed to generate workflow: {e}&quot;)
</code></pre>
<p><strong>Task 5.3: Agent Communication Visualization</strong> <em>(2 days)</em></p>
<pre><code class="language-python"># ui/agent_visualization.py
import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
from typing import List, Dict

def visualize_workflow_execution(workflow_trace: List[Dict]):
    &quot;&quot;&quot;Visualize agent interactions during workflow execution&quot;&quot;&quot;

    st.subheader(&quot;Workflow Execution Trace&quot;)

    # Create directed graph
    G = nx.DiGraph()

    for step in workflow_trace:
        agent_id = step[&quot;agent_id&quot;]
        capability = step[&quot;capability&quot;]

        # Add node
        G.add_node(agent_id, label=capability)

        # Add edges (agent communication)
        if &quot;messages_sent&quot; in step:
            for target in step[&quot;messages_sent&quot;]:
                G.add_edge(agent_id, target)

    # Draw graph
    fig, ax = plt.subplots(figsize=(12, 8))
    pos = nx.spring_layout(G)

    nx.draw(G, pos, ax=ax, with_labels=True, node_color='lightblue',
            node_size=3000, font_size=10, font_weight='bold',
            arrows=True, edge_color='gray')

    st.pyplot(fig)

    # Timeline view
    st.subheader(&quot;Execution Timeline&quot;)

    for i, step in enumerate(workflow_trace):
        col1, col2, col3 = st.columns([1, 3, 2])

        with col1:
            st.write(f&quot;**Step {i+1}**&quot;)

        with col2:
            st.write(f&quot;{step['agent_id']}: {step['capability']}&quot;)

        with col3:
            duration = step.get(&quot;duration&quot;, 0)
            st.write(f&quot;⏱️ {duration:.2f}s&quot;)

        if step.get(&quot;status&quot;) == &quot;error&quot;:
            st.error(f&quot;Error: {step.get('error_message')}&quot;)
</code></pre>
<p><strong>Deliverables:</strong>
- <code>ui/agentic_dashboard.py</code> - Agent monitoring dashboard
- <code>ui/workflow_creator.py</code> - Natural language workflow UI
- <code>ui/agent_visualization.py</code> - Visualization components
- Updated main Streamlit app with new UI components</p>
<hr />
<h3><strong>PHASE 6: Testing &amp; Documentation (1 week)</strong></h3>
<h4><strong>Week 11: Comprehensive Testing</strong></h4>
<p><strong>Task 6.1: Unit Tests</strong> <em>(2 days)</em>
- Test all agent classes
- Test orchestrator logic
- Test MCP integration
- Test memory systems</p>
<p><strong>Task 6.2: Integration Tests</strong> <em>(2 days)</em>
- End-to-end workflow tests
- Multi-agent coordination tests
- MCP server interaction tests
- Performance benchmarks</p>
<p><strong>Task 6.3: Documentation</strong> <em>(3 days)</em></p>
<p>Create comprehensive documentation:</p>
<ol>
<li><strong>Agent Development Guide</strong></li>
<li>How to create new agents</li>
<li>Agent lifecycle</li>
<li>
<p>Best practices</p>
</li>
<li>
<p><strong>Workflow Creation Guide</strong></p>
</li>
<li>Workflow DSL reference</li>
<li>Natural language examples</li>
<li>
<p>Debugging workflows</p>
</li>
<li>
<p><strong>MCP Integration Guide</strong></p>
</li>
<li>Supported MCP servers</li>
<li>Custom MCP server development</li>
<li>
<p>Security considerations</p>
</li>
<li>
<p><strong>API Reference</strong></p>
</li>
<li>BaseAgent API</li>
<li>Orchestrator API</li>
<li>
<p>MCP Manager API</p>
</li>
<li>
<p><strong>Migration Guide</strong></p>
</li>
<li>Migrating existing code to agentic framework</li>
<li>Backward compatibility notes</li>
<li>Troubleshooting</li>
</ol>
<p><strong>Deliverables:</strong>
- Complete test suite (&gt;80% coverage)
- Documentation website (Markdown + MkDocs)
- Example notebooks (Jupyter)
- Video tutorials (optional)</p>
<hr />
<h2>7. Implementation Milestones</h2>
<h3>Milestone 1: Foundation Complete (Week 2)</h3>
<ul>
<li>✅ Base agent infrastructure</li>
<li>✅ Agent registry operational</li>
<li>✅ Message bus functional</li>
<li>✅ Existing agents refactored</li>
</ul>
<p><strong>Success Criteria:</strong>
- All existing workflows work with refactored agents
- &lt;5% performance degradation
- Unit tests pass</p>
<h3>Milestone 2: Orchestration Ready (Week 4-5)</h3>
<ul>
<li>✅ Master orchestrator implemented</li>
<li>✅ Workflow DSL defined</li>
<li>✅ Natural language translation working</li>
<li>✅ Pre-defined workflows operational</li>
</ul>
<p><strong>Success Criteria:</strong>
- Can execute 3+ complex workflows end-to-end
- Workflow execution time &lt; 10% overhead vs. manual
- Natural language accuracy &gt;80%</p>
<h3>Milestone 3: MCP Integrated (Week 6-7)</h3>
<ul>
<li>✅ MCP client manager operational</li>
<li>✅ 3+ MCP servers connected</li>
<li>✅ Custom MCP server deployed</li>
<li>✅ Agents using MCP tools</li>
</ul>
<p><strong>Success Criteria:</strong>
- Can use external MCP tools in workflows
- Graceful fallback when MCP unavailable
- &lt;2s latency for MCP tool calls</p>
<h3>Milestone 4: Production Ready (Week 9-10)</h3>
<ul>
<li>✅ Advanced features implemented</li>
<li>✅ UI fully integrated</li>
<li>✅ Performance optimized</li>
<li>✅ Documentation complete</li>
</ul>
<p><strong>Success Criteria:</strong>
- All tests passing
- Performance benchmarks met
- Documentation reviewed
- Beta users onboarded</p>
<hr />
<h2>8. Success Metrics</h2>
<h3>Technical Metrics</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Baseline</th>
<th>Target</th>
<th>Measurement Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Workflow Automation Rate</strong></td>
<td>40%</td>
<td>85%</td>
<td>% of steps automated</td>
</tr>
<tr>
<td><strong>Agent Response Time</strong></td>
<td>N/A</td>
<td>&lt;2s (avg)</td>
<td>Performance monitor</td>
</tr>
<tr>
<td><strong>System Throughput</strong></td>
<td>1 test/session</td>
<td>5-10 parallel tests</td>
<td>Load testing</td>
</tr>
<tr>
<td><strong>Error Rate</strong></td>
<td>~15%</td>
<td>&lt;5%</td>
<td>Error tracking</td>
</tr>
<tr>
<td><strong>Code Coverage</strong></td>
<td>45%</td>
<td>&gt;80%</td>
<td>pytest-cov</td>
</tr>
<tr>
<td><strong>Agent Reusability</strong></td>
<td>30%</td>
<td>&gt;90%</td>
<td>Code analysis</td>
</tr>
</tbody>
</table>
<h3>User Experience Metrics</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Baseline</th>
<th>Target</th>
<th>Measurement Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Setup Time per Test</strong></td>
<td>15-20 min</td>
<td>&lt;3 min</td>
<td>User timing studies</td>
</tr>
<tr>
<td><strong>User Satisfaction</strong></td>
<td>N/A</td>
<td>&gt;4.0/5.0</td>
<td>Surveys</td>
</tr>
<tr>
<td><strong>Feature Adoption</strong></td>
<td>N/A</td>
<td>&gt;60%</td>
<td>Analytics</td>
</tr>
<tr>
<td><strong>Training Time</strong></td>
<td>2-3 days</td>
<td>&lt;1 day</td>
<td>User feedback</td>
</tr>
</tbody>
</table>
<h3>Business Metrics</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Target</th>
<th>Measurement Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Development Velocity</strong></td>
<td>+50% faster feature development</td>
<td>Sprint velocity</td>
</tr>
<tr>
<td><strong>Maintenance Burden</strong></td>
<td>-30% time on bug fixes</td>
<td>Time tracking</td>
</tr>
<tr>
<td><strong>Integration Partners</strong></td>
<td>5+ MCP integrations</td>
<td>Partnership count</td>
</tr>
<tr>
<td><strong>API Usage</strong></td>
<td>1000+ agent tasks/day</td>
<td>Usage analytics</td>
</tr>
</tbody>
</table>
<hr />
<h2>9. Appendices</h2>
<h3>Appendix A: Agent Capability Matrix</h3>
<table>
<thead>
<tr>
<th>Agent</th>
<th>Capabilities</th>
<th>Input</th>
<th>Output</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ConfigurationAgent</strong></td>
<td>select_ai_mode, detect_instruments, configure_port</td>
<td>mode, filters</td>
<td>model_ready, instruments</td>
<td>Low</td>
</tr>
<tr>
<td><strong>LocationAgent</strong></td>
<td>detect_location, fetch_operator_tables</td>
<td>lat/lon, country</td>
<td>frequency_map</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>AnalysisAgent</strong></td>
<td>analyze_spectrum, detect_anomalies, generate_insights</td>
<td>spectrum_data</td>
<td>analysis_results</td>
<td>High</td>
</tr>
<tr>
<td><strong>TinySAAgent</strong></td>
<td>configure_scan, execute_scan, calibrate</td>
<td>freq_range, rbw</td>
<td>spectrum_data</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>ViaviAgent</strong></td>
<td>configure_5gnr, run_test, fetch_results</td>
<td>band, bw</td>
<td>test_results</td>
<td>High</td>
</tr>
<tr>
<td><strong>ReportAgent</strong></td>
<td>generate_pdf, generate_html, export_csv</td>
<td>test_data, template</td>
<td>report_file</td>
<td>Low</td>
</tr>
<tr>
<td><strong>StorageAgent</strong></td>
<td>save_results, load_results, backup</td>
<td>data, path</td>
<td>file_path</td>
<td>Low</td>
</tr>
</tbody>
</table>
<h3>Appendix B: MCP Server Comparison</h3>
<table>
<thead>
<tr>
<th>Server</th>
<th>Protocol</th>
<th>Language</th>
<th>Use Case</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>filesystem</strong></td>
<td>stdio</td>
<td>Node.js</td>
<td>File access</td>
<td>Fast, reliable</td>
<td>Limited permissions</td>
</tr>
<tr>
<td><strong>postgres</strong></td>
<td>stdio</td>
<td>Node.js</td>
<td>Database</td>
<td>SQL support</td>
<td>Requires DB setup</td>
</tr>
<tr>
<td><strong>slack</strong></td>
<td>stdio</td>
<td>Node.js</td>
<td>Notifications</td>
<td>Easy integration</td>
<td>Rate limits</td>
</tr>
<tr>
<td><strong>Custom Spectrum ML</strong></td>
<td>stdio</td>
<td>Python</td>
<td>ML analysis</td>
<td>Tailored models</td>
<td>Maintenance burden</td>
</tr>
</tbody>
</table>
<h3>Appendix C: Risk Mitigation Strategies</h3>
<h4>Risk: Breaking Changes to Existing Workflows</h4>
<p><strong>Mitigation:</strong>
1. Maintain backward compatibility layer
2. Feature flags for new agentic features
3. Gradual migration path
4. Comprehensive regression testing</p>
<p><strong>Rollback Plan:</strong>
- Keep original code in <code>legacy/</code> folder
- Git tags for each phase
- Database migrations are reversible</p>
<h4>Risk: MCP Server Unavailability</h4>
<p><strong>Mitigation:</strong>
1. Implement circuit breakers
2. Local fallback implementations
3. Caching of MCP responses
4. Health checks before critical operations</p>
<p><strong>Fallback Strategy:</strong>
- All agents have local implementations
- Gracefully degrade features
- Queue requests for retry</p>
<h4>Risk: Performance Degradation</h4>
<p><strong>Mitigation:</strong>
1. Continuous performance monitoring
2. Load testing at each phase
3. Profiling and optimization
4. Caching strategies</p>
<p><strong>Optimization Tactics:</strong>
- Lazy loading of agents
- Connection pooling for instruments
- Async/await for parallel execution
- Memoization of expensive operations</p>
<h3>Appendix D: Code Review Checklist</h3>
<p>For each pull request during implementation:</p>
<ul>
<li>[ ] Code follows existing style guide</li>
<li>[ ] Unit tests added/updated (&gt;80% coverage)</li>
<li>[ ] Integration tests pass</li>
<li>[ ] Documentation updated</li>
<li>[ ] Performance benchmarks run (no regression)</li>
<li>[ ] Security review completed (if applicable)</li>
<li>[ ] Backward compatibility verified</li>
<li>[ ] MCP integration tested (if applicable)</li>
<li>[ ] Error handling robust</li>
<li>[ ] Logging adequate for debugging</li>
</ul>
<h3>Appendix E: Glossary</h3>
<table>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Agent</strong></td>
<td>Autonomous software component with specific capabilities</td>
</tr>
<tr>
<td><strong>Orchestrator</strong></td>
<td>Meta-agent that coordinates other agents</td>
</tr>
<tr>
<td><strong>Capability</strong></td>
<td>A specific function an agent can perform</td>
</tr>
<tr>
<td><strong>Workflow</strong></td>
<td>Sequence of agent tasks to accomplish a goal</td>
</tr>
<tr>
<td><strong>MCP</strong></td>
<td>Model Context Protocol - standardized agent communication</td>
</tr>
<tr>
<td><strong>Tool</strong></td>
<td>External function accessible via MCP</td>
</tr>
<tr>
<td><strong>Resource</strong></td>
<td>Read-only data accessible via MCP</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>Wrapper for specific instrument communication</td>
</tr>
<tr>
<td><strong>Message Bus</strong></td>
<td>Pub/sub system for agent communication</td>
</tr>
<tr>
<td><strong>Agent Memory</strong></td>
<td>RAG-based system for agent learning</td>
</tr>
</tbody>
</table>
<hr />
<h2>Conclusion</h2>
<p>Transforming EnnoiaCAT into a full agentic framework is a <strong>medium-high complexity</strong> effort with <strong>high return on investment</strong>. The platform already has strong foundational elements (adapter pattern, existing agents, modular design) that significantly reduce implementation risk.</p>
<h3>Key Recommendations:</h3>
<ol>
<li><strong>Start with Phase 1</strong> (Foundation) to validate the approach with minimal risk</li>
<li><strong>Prioritize MCP integration early</strong> (Phase 3) to unlock ecosystem benefits</li>
<li><strong>Iterate on user feedback</strong> throughout Phases 4-5</li>
<li><strong>Invest in comprehensive testing</strong> to maintain reliability</li>
</ol>
<h3>Expected Outcomes:</h3>
<ul>
<li><strong>85% workflow automation</strong> (vs. 40% currently)</li>
<li><strong>5-10x throughput</strong> via parallel agent execution</li>
<li><strong>90% agent reusability</strong> enabling rapid feature development</li>
<li><strong>Ecosystem integration</strong> via MCP opening new use cases</li>
</ul>
<h3>Next Steps:</h3>
<ol>
<li><strong>Review and approve</strong> this analysis document</li>
<li><strong>Allocate resources</strong> for 9-11 week implementation</li>
<li><strong>Set up development environment</strong> and create feature branch</li>
<li><strong>Begin Phase 0</strong> (Preparation &amp; Planning)</li>
<li><strong>Schedule weekly checkpoints</strong> to track progress</li>
</ol>
<hr />
<p><strong>Document End</strong></p>